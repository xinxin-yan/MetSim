{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "mount_file_id": "1QQvb0DOHY7wA3LvXJpjCZlx9KQJj4SjQ",
      "authorship_tag": "ABX9TyNoQ2E5gU3j53FFw3tfdw8W",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/xinxin-yan/MetSim/blob/main/Comp_Ling_final_assessment_KYCS5.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 0. Encironment preparation\n",
        "ÂÆâË£Ö transformers, datasets, torch Á≠â‰æùËµñÂ∫ì„ÄÇ"
      ],
      "metadata": {
        "id": "IiYBxe0dGsgy"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "RWHyzbdWa3Du",
        "outputId": "edda6346-86d9-4095-eedf-79d14015cb2c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: spacy in /usr/local/lib/python3.11/dist-packages (3.8.5)\n",
            "Requirement already satisfied: benepar in /usr/local/lib/python3.11/dist-packages (0.2.0)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.11/dist-packages (from spacy) (3.0.12)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (1.0.5)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (1.0.12)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.11/dist-packages (from spacy) (2.0.11)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.11/dist-packages (from spacy) (3.0.9)\n",
            "Requirement already satisfied: thinc<8.4.0,>=8.3.4 in /usr/local/lib/python3.11/dist-packages (from spacy) (8.3.6)\n",
            "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.11/dist-packages (from spacy) (1.1.3)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.11/dist-packages (from spacy) (2.5.1)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.11/dist-packages (from spacy) (2.0.10)\n",
            "Requirement already satisfied: weasel<0.5.0,>=0.1.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (0.4.1)\n",
            "Requirement already satisfied: typer<1.0.0,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (0.15.2)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (4.67.1)\n",
            "Requirement already satisfied: numpy>=1.19.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (2.0.2)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (2.32.3)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /usr/local/lib/python3.11/dist-packages (from spacy) (2.11.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from spacy) (3.1.6)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from spacy) (75.2.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (24.2)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (3.5.0)\n",
            "Requirement already satisfied: nltk>=3.2 in /usr/local/lib/python3.11/dist-packages (from benepar) (3.9.1)\n",
            "Requirement already satisfied: torch>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from benepar) (2.6.0+cu124)\n",
            "Requirement already satisfied: torch-struct>=0.5 in /usr/local/lib/python3.11/dist-packages (from benepar) (0.5)\n",
            "Requirement already satisfied: tokenizers>=0.9.4 in /usr/local/lib/python3.11/dist-packages (from benepar) (0.21.1)\n",
            "Requirement already satisfied: transformers>=4.2.2 in /usr/local/lib/python3.11/dist-packages (from transformers[tokenizers,torch]>=4.2.2->benepar) (4.51.1)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.11/dist-packages (from benepar) (5.29.4)\n",
            "Requirement already satisfied: sentencepiece>=0.1.91 in /usr/local/lib/python3.11/dist-packages (from benepar) (0.2.0)\n",
            "Requirement already satisfied: language-data>=1.2 in /usr/local/lib/python3.11/dist-packages (from langcodes<4.0.0,>=3.2.0->spacy) (1.3.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.11/dist-packages (from nltk>=3.2->benepar) (8.1.8)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.11/dist-packages (from nltk>=3.2->benepar) (1.4.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.11/dist-packages (from nltk>=3.2->benepar) (2024.11.6)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.1 in /usr/local/lib/python3.11/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (2.33.1)\n",
            "Requirement already satisfied: typing-extensions>=4.12.2 in /usr/local/lib/python3.11/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (4.13.1)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (0.4.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2025.1.31)\n",
            "Requirement already satisfied: blis<1.4.0,>=1.3.0 in /usr/local/lib/python3.11/dist-packages (from thinc<8.4.0,>=8.3.4->spacy) (1.3.0)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.11/dist-packages (from thinc<8.4.0,>=8.3.4->spacy) (0.1.5)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.16.4 in /usr/local/lib/python3.11/dist-packages (from tokenizers>=0.9.4->benepar) (0.30.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch>=1.6.0->benepar) (3.18.0)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=1.6.0->benepar) (3.4.2)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch>=1.6.0->benepar) (2025.3.2)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.6.0->benepar) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.6.0->benepar) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.6.0->benepar) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch>=1.6.0->benepar) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch>=1.6.0->benepar) (12.4.5.8)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch>=1.6.0->benepar) (11.2.1.3)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch>=1.6.0->benepar) (10.3.5.147)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch>=1.6.0->benepar) (11.6.1.9)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch>=1.6.0->benepar) (12.3.1.170)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.6.0->benepar) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=1.6.0->benepar) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.6.0->benepar) (12.4.127)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.6.0->benepar) (12.4.127)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.6.0->benepar) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=1.6.0->benepar) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=1.6.0->benepar) (1.3.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers>=4.2.2->transformers[tokenizers,torch]>=4.2.2->benepar) (6.0.2)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers>=4.2.2->transformers[tokenizers,torch]>=4.2.2->benepar) (0.5.3)\n",
            "Requirement already satisfied: accelerate>=0.26.0 in /usr/local/lib/python3.11/dist-packages (from transformers[tokenizers,torch]>=4.2.2->benepar) (1.5.2)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0.0,>=0.3.0->spacy) (1.5.4)\n",
            "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0.0,>=0.3.0->spacy) (13.9.4)\n",
            "Requirement already satisfied: cloudpathlib<1.0.0,>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from weasel<0.5.0,>=0.1.0->spacy) (0.21.0)\n",
            "Requirement already satisfied: smart-open<8.0.0,>=5.2.1 in /usr/local/lib/python3.11/dist-packages (from weasel<0.5.0,>=0.1.0->spacy) (7.1.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->spacy) (3.0.2)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.11/dist-packages (from accelerate>=0.26.0->transformers[tokenizers,torch]>=4.2.2->benepar) (5.9.5)\n",
            "Requirement already satisfied: marisa-trie>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy) (1.2.1)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (2.18.0)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.11/dist-packages (from smart-open<8.0.0,>=5.2.1->weasel<0.5.0,>=0.1.0->spacy) (1.17.2)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (0.1.2)\n",
            "Collecting en-core-web-sm==3.8.0\n",
            "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.8.0/en_core_web_sm-3.8.0-py3-none-any.whl (12.8 MB)\n",
            "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m12.8/12.8 MB\u001b[0m \u001b[31m99.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[38;5;2m‚úî Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('en_core_web_sm')\n",
            "\u001b[38;5;3m‚ö† Restart to reload dependencies\u001b[0m\n",
            "If you are in a Jupyter or Colab notebook, you may need to restart Python in\n",
            "order to load all the package's dependencies. You can do this by selecting the\n",
            "'Restart kernel' or 'Restart runtime' option.\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import pandas as pd\n",
        "from transformers import RobertaTokenizer, RobertaModel\n",
        "from torch.optim import AdamW\n",
        "from sklearn.metrics import f1_score\n",
        "import numpy as np\n",
        "import os\n",
        "!pip install spacy benepar\n",
        "!python -m spacy download en_core_web_sm"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 1. Data preprocessing\n",
        "Âä†ËΩΩ VUA Êï∞ÊçÆÈõÜÔºåÂπ∂ËΩ¨Êç¢‰∏∫ÈÄÇÁî®‰∫é RoBERTa ÁöÑÊ†ºÂºèÔºàTokenizationÔºâ„ÄÇ\n",
        "\n",
        "Â§ÑÁêÜÊ†áÁ≠æÔºàÈöêÂñª/ÈùûÈöêÂñªÂàÜÁ±ª‰ªªÂä°Ôºâ„ÄÇ\n",
        "\n"
      ],
      "metadata": {
        "id": "6QDenszfkjuO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset Class\n",
        "class MetaphorDataset(Dataset):\n",
        "    def __init__(self, file_path, tokenizer_name=\"roberta-base\", max_len=128):\n",
        "        self.data = pd.read_csv(file_path, sep='\\t', header=None)\n",
        "        self.tokenizer = RobertaTokenizer.from_pretrained(tokenizer_name)\n",
        "        self.max_len = max_len\n",
        "\n",
        "        self.sentences = self.data[2].tolist()\n",
        "        self.fgpos = self.data[4].tolist()\n",
        "        self.target_indices = self.data[5].tolist()\n",
        "        self.labels = self.data[6].tolist()\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.sentences)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        sentence = str(self.sentences[idx])\n",
        "        fgpos_tag = str(self.fgpos[idx])\n",
        "        target_index = int(self.target_indices[idx])\n",
        "        label = int(self.labels[idx])\n",
        "\n",
        "        words = sentence.split()\n",
        "        target_word = words[target_index]\n",
        "\n",
        "        marked_sentence = f\"<s> {' '.join(words)} </s> {fgpos_tag} </s>\"\n",
        "        sent_encoding = self.tokenizer(marked_sentence, padding='max_length', truncation=True, max_length=self.max_len, return_tensors='pt')\n",
        "\n",
        "        isolated_word = f\"<s> {target_word} </s>\"\n",
        "        word_encoding = self.tokenizer(isolated_word, padding='max_length', truncation=True, max_length=self.max_len, return_tensors='pt')\n",
        "\n",
        "        tokenized_words = self.tokenizer.tokenize(\" \".join(words))\n",
        "        try:\n",
        "            token_idx = tokenized_words.index(self.tokenizer.tokenize(target_word)[0]) + 1\n",
        "        except ValueError:\n",
        "            token_idx = 1\n",
        "\n",
        "        return {\n",
        "            \"sentence_input_ids\": sent_encoding[\"input_ids\"].squeeze(0),\n",
        "            \"sentence_attention_mask\": sent_encoding[\"attention_mask\"].squeeze(0),\n",
        "            \"word_input_ids\": word_encoding[\"input_ids\"].squeeze(0),\n",
        "            \"word_attention_mask\": word_encoding[\"attention_mask\"].squeeze(0),\n",
        "            \"target_token_index\": torch.tensor(token_idx),\n",
        "            \"label\": torch.tensor(label, dtype=torch.float)\n",
        "        }"
      ],
      "metadata": {
        "id": "q7oZus_9koIg"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2. Model building\n",
        "Âä†ËΩΩ RoBERTa È¢ÑËÆ≠ÁªÉÊ®°Âûã„ÄÇ\n",
        "\n",
        "‰øÆÊîπÊúÄÂêé‰∏ÄÂ±Ç‰ª•ÈÄÇÂ∫î‰∫åÂàÜÁ±ª‰ªªÂä°ÔºàÈöêÂñªÊ£ÄÊµãÔºâ„ÄÇ"
      ],
      "metadata": {
        "id": "-hOj7S_LkogP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Model using MIP + SPV as in MelBERT\n",
        "class MelBERT(nn.Module):\n",
        "    def __init__(self, hidden_size=768, classifier_hidden=256):\n",
        "        super(MelBERT, self).__init__()\n",
        "        self.encoder = RobertaModel.from_pretrained(\"roberta-base\", add_pooling_layer=False)\n",
        "\n",
        "        self.mip_layer = nn.Linear(2 * hidden_size, classifier_hidden)\n",
        "        self.spv_layer = nn.Linear(2 * hidden_size, classifier_hidden)\n",
        "        self.classifier = nn.Linear(2 * classifier_hidden, 1)\n",
        "        self.dropout = nn.Dropout(0.1)\n",
        "        self.sigmoid = nn.Sigmoid()\n",
        "\n",
        "        self._init_weights(self.mip_layer)\n",
        "        self._init_weights(self.spv_layer)\n",
        "        self._init_weights(self.classifier)\n",
        "\n",
        "    def _init_weights(self, module):\n",
        "        if isinstance(module, nn.Linear):\n",
        "            nn.init.xavier_uniform_(module.weight)\n",
        "            if module.bias is not None:\n",
        "                module.bias.data.zero_()\n",
        "\n",
        "    def forward(self, sentence_input_ids, sentence_attention_mask, word_input_ids, word_attention_mask, target_token_index):\n",
        "        sent_output = self.encoder(input_ids=sentence_input_ids, attention_mask=sentence_attention_mask)\n",
        "        word_output = self.encoder(input_ids=word_input_ids, attention_mask=word_attention_mask)\n",
        "\n",
        "        vS = sent_output.last_hidden_state[:, 0, :]  # sentence CLS\n",
        "        vS_t = torch.stack([sent_output.last_hidden_state[i, idx, :] for i, idx in enumerate(target_token_index)])\n",
        "        vT = word_output.last_hidden_state[:, 0, :]  # isolated CLS\n",
        "\n",
        "        h_mip = self.dropout(torch.relu(self.mip_layer(torch.cat([vT, vS_t], dim=1))))\n",
        "        h_spv = self.dropout(torch.relu(self.spv_layer(torch.cat([vS, vS_t], dim=1))))\n",
        "\n",
        "        logits = self.classifier(torch.cat([h_mip, h_spv], dim=1)).squeeze(1)\n",
        "        return self.sigmoid(logits)\n"
      ],
      "metadata": {
        "id": "OUSDM1g3ks0S"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 3. Model training\n",
        "ËÆæÂÆö‰ºòÂåñÂô®„ÄÅÊçüÂ§±ÂáΩÊï∞ÔºàÂ¶Ç‰∫§ÂèâÁÜµÊçüÂ§±Ôºâ„ÄÇ\n",
        "\n",
        "‰ΩøÁî® GPU ËÆ≠ÁªÉÔºåÂπ∂Ë∞ÉÊï¥Ë∂ÖÂèÇÊï∞ÔºàÂ≠¶‰π†Áéá„ÄÅÊâπÈáèÂ§ßÂ∞èÁ≠âÔºâ„ÄÇ"
      ],
      "metadata": {
        "id": "At7QvEeektNf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Training Loop\n",
        "def train(model, dataloader, optimizer, criterion, device):\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "    for batch in dataloader:\n",
        "        for k in batch:\n",
        "            batch[k] = batch[k].to(device)\n",
        "\n",
        "        outputs = model(batch['sentence_input_ids'], batch['sentence_attention_mask'],\n",
        "                        batch['word_input_ids'], batch['word_attention_mask'], batch['target_token_index'])\n",
        "        loss = criterion(outputs, batch['label'])\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        optimizer.zero_grad()\n",
        "        total_loss += loss.item()\n",
        "    return total_loss / len(dataloader)"
      ],
      "metadata": {
        "id": "JPiX-qfpkvZY"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 4. Model evaluation\n",
        "Âú®ÊµãËØïÈõÜ‰∏äËøõË°åËØÑ‰º∞ÔºåËÆ°ÁÆó F1-score, accuracy Á≠âÊåáÊ†á„ÄÇ"
      ],
      "metadata": {
        "id": "Z-y__Nhtkvy0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluation Loop\n",
        "def evaluate(model, dataloader, device):\n",
        "    model.eval()\n",
        "    preds, labels = [], []\n",
        "    with torch.no_grad():\n",
        "        for batch in dataloader:\n",
        "            for k in batch:\n",
        "                batch[k] = batch[k].to(device)\n",
        "            outputs = model(batch['sentence_input_ids'], batch['sentence_attention_mask'],\n",
        "                            batch['word_input_ids'], batch['word_attention_mask'], batch['target_token_index'])\n",
        "            preds.extend(outputs.cpu().numpy())\n",
        "            labels.extend(batch['label'].cpu().numpy())\n",
        "    preds = np.array(preds) > 0.5\n",
        "    return f1_score(labels, preds), preds"
      ],
      "metadata": {
        "id": "6fXr3IoYkz6e"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 5. Main"
      ],
      "metadata": {
        "id": "ioLhzFrTk0OU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rX-dwv-JFshK",
        "outputId": "68b113f2-8214-4546-af41-3f077333d7bc"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Main Entry\n",
        "train_file = \"/content/drive/MyDrive/Colab Notebooks/cl dataset/train_novelty_labels.tsv\"\n",
        "test_file = \"/content/drive/MyDrive/Colab Notebooks/cl dataset/test_novelty_labels.tsv\"\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "train_dataset = MetaphorDataset(train_file)\n",
        "test_dataset = MetaphorDataset(test_file)\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
        "test_loader = DataLoader(test_dataset, batch_size=32)\n",
        "\n",
        "model = MelBERT().to(device)\n",
        "optimizer = AdamW(model.parameters(), lr=2e-5)\n",
        "criterion = nn.BCELoss()\n",
        "\n",
        "for epoch in range(5):\n",
        "    train_loss = train(model, train_loader, optimizer, criterion, device)\n",
        "    f1, _ = evaluate(model, test_loader, device)\n",
        "    print(f\"Epoch {epoch+1} - Train Loss: {train_loss:.4f}, Test F1: {f1:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 332
        },
        "id": "qVbYM4o_lBbs",
        "outputId": "cd05b399-010b-4d11-b41b-ccccd80c4dff"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: 'config.json'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-8-64fbada5199f>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"__main__\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m     \u001b[0mconfig\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_config\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"config.json\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0mtrain_file\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"train_file\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-8-64fbada5199f>\u001b[0m in \u001b[0;36mload_config\u001b[0;34m(config_path)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Main Entry\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mload_config\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig_path\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"/content/drive/MyDrive/Colab Notebooks/cl dataset/config.json\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'r'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mjson\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'config.json'"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# extraction"
      ],
      "metadata": {
        "id": "yHvOQhEGDfre"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "import benepar\n",
        "\n",
        "# Âä†ËΩΩ parser\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "if not benepar.is_loaded(\"benepar_en3\"):\n",
        "    benepar.download(\"benepar_en3\")\n",
        "nlp.add_pipe(\"benepar\", config={\"model\": \"benepar_en3\"})\n",
        "\n",
        "def extract_constituent_phrase(sentence, target_index, phrase_type=\"NP_or_VP\"):\n",
        "    \"\"\"\n",
        "    phrase_type = \"NP_or_VP\": Ëá™Âä®Âà§Êñ≠\n",
        "                 \"NP\": Âº∫Âà∂Âè™ÊäΩNP\n",
        "                 \"VP\": Âº∫Âà∂Âè™ÊäΩVP\n",
        "    \"\"\"\n",
        "    doc = nlp(sentence)\n",
        "    words = sentence.split()\n",
        "    target_word = words[target_index]\n",
        "\n",
        "    for sent in doc.sents:\n",
        "        tree = sent._.parse_string\n",
        "        parsed = sent._.constituent\n",
        "\n",
        "        # ÈÅçÂéÜ constituent Ê†ëÊü•ÊâæÂåÖÂê´ target ÁöÑÊúÄÂ∞è NP/VP\n",
        "        for constituent in parsed.subtrees:\n",
        "            if constituent.label_ in {\"NP\", \"VP\"}:\n",
        "                leaves = list(constituent.leaves())\n",
        "                leaf_text = [t.text for t in leaves]\n",
        "                if target_word in leaf_text:\n",
        "                    if phrase_type == \"NP_or_VP\":\n",
        "                        return \" \".join(leaf_text)\n",
        "                    elif phrase_type == \"NP\" and constituent.label_ == \"NP\":\n",
        "                        return \" \".join(leaf_text)\n",
        "                    elif phrase_type == \"VP\" and constituent.label_ == \"VP\":\n",
        "                        return \" \".join(leaf_text)\n",
        "    return target_word  # fallback\n"
      ],
      "metadata": {
        "id": "NTKE66QMDckE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def guess_phrase_type(pos_tag):\n",
        "    if pos_tag.startswith(\"V\"):\n",
        "        return \"VP\"\n",
        "    elif pos_tag.startswith(\"N\"):\n",
        "        return \"NP\"\n",
        "    else:\n",
        "        return \"NP_or_VP\"\n",
        "\n",
        "\n",
        "# ËæìÂá∫Ê®°ÂûãÈ¢ÑÊµã‰∏∫ metaphor ÁöÑËØçÂèäÂÖ∂ÊâÄÂú®Áü≠ËØ≠\n",
        "for i in range(len(test_dataset)):\n",
        "    item = test_dataset[i]\n",
        "    sentence = test_dataset.sentences[i]\n",
        "    idx = int(test_dataset.target_indices[i])\n",
        "    pos = test_dataset.pos[i]\n",
        "    if preds[i]:  # Ë¢´È¢ÑÊµã‰∏∫ metaphor\n",
        "        phrase = extract_constituent_phrase(sentence, idx, guess_phrase_type(pos))\n",
        "        print(f\"üìç [{phrase}]  ‚Üê from sentence: {sentence}\")\n"
      ],
      "metadata": {
        "id": "O_2BQjFJDrxe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# save"
      ],
      "metadata": {
        "id": "-4ZYHFcgD0cU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import csv\n",
        "\n",
        "output_path = \"melbert_metaphor_predictions.tsv\"\n",
        "rows = []\n",
        "\n",
        "for i in range(len(test_dataset)):\n",
        "    sentence = test_dataset.sentences[i]\n",
        "    idx = int(test_dataset.target_indices[i])\n",
        "    pos = test_dataset.pos[i]\n",
        "    label = int(test_dataset.labels[i])\n",
        "    pred = int(preds[i])\n",
        "\n",
        "    target_word = sentence.split()[idx]\n",
        "    phrase = extract_constituent_phrase(sentence, idx, guess_phrase_type(pos))\n",
        "\n",
        "    rows.append({\n",
        "        \"sentence\": sentence,\n",
        "        \"target_word\": target_word,\n",
        "        \"target_index\": idx,\n",
        "        \"POS\": pos,\n",
        "        \"true_label\": label,\n",
        "        \"predicted_label\": pred,\n",
        "        \"extracted_phrase\": phrase\n",
        "    })\n",
        "\n",
        "# ‰øùÂ≠ò‰∏∫ TSV\n",
        "with open(output_path, \"w\", newline=\"\", encoding=\"utf-8\") as f:\n",
        "    writer = csv.DictWriter(f, fieldnames=rows[0].keys(), delimiter=\"\\t\")\n",
        "    writer.writeheader()\n",
        "    writer.writerows(rows)\n",
        "\n",
        "print(f\"‚úÖ ‰øùÂ≠òÊàêÂäüÔºÅÈ¢ÑÊµãÁªìÊûúÂ∑≤ÂÜôÂÖ•Ôºö{output_path}\")"
      ],
      "metadata": {
        "id": "DqNWT80ED1ye"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}