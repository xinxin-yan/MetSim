{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "mount_file_id": "1QQvb0DOHY7wA3LvXJpjCZlx9KQJj4SjQ",
      "authorship_tag": "ABX9TyNoQ2E5gU3j53FFw3tfdw8W",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/xinxin-yan/MetSim/blob/main/Comp_Ling_final_assessment_KYCS5.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 0. Encironment preparation\n",
        "安装 transformers, datasets, torch 等依赖库。"
      ],
      "metadata": {
        "id": "IiYBxe0dGsgy"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "RWHyzbdWa3Du",
        "outputId": "edda6346-86d9-4095-eedf-79d14015cb2c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: spacy in /usr/local/lib/python3.11/dist-packages (3.8.5)\n",
            "Requirement already satisfied: benepar in /usr/local/lib/python3.11/dist-packages (0.2.0)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.11/dist-packages (from spacy) (3.0.12)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (1.0.5)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (1.0.12)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.11/dist-packages (from spacy) (2.0.11)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.11/dist-packages (from spacy) (3.0.9)\n",
            "Requirement already satisfied: thinc<8.4.0,>=8.3.4 in /usr/local/lib/python3.11/dist-packages (from spacy) (8.3.6)\n",
            "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.11/dist-packages (from spacy) (1.1.3)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.11/dist-packages (from spacy) (2.5.1)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.11/dist-packages (from spacy) (2.0.10)\n",
            "Requirement already satisfied: weasel<0.5.0,>=0.1.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (0.4.1)\n",
            "Requirement already satisfied: typer<1.0.0,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (0.15.2)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (4.67.1)\n",
            "Requirement already satisfied: numpy>=1.19.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (2.0.2)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (2.32.3)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /usr/local/lib/python3.11/dist-packages (from spacy) (2.11.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from spacy) (3.1.6)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from spacy) (75.2.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (24.2)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (3.5.0)\n",
            "Requirement already satisfied: nltk>=3.2 in /usr/local/lib/python3.11/dist-packages (from benepar) (3.9.1)\n",
            "Requirement already satisfied: torch>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from benepar) (2.6.0+cu124)\n",
            "Requirement already satisfied: torch-struct>=0.5 in /usr/local/lib/python3.11/dist-packages (from benepar) (0.5)\n",
            "Requirement already satisfied: tokenizers>=0.9.4 in /usr/local/lib/python3.11/dist-packages (from benepar) (0.21.1)\n",
            "Requirement already satisfied: transformers>=4.2.2 in /usr/local/lib/python3.11/dist-packages (from transformers[tokenizers,torch]>=4.2.2->benepar) (4.51.1)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.11/dist-packages (from benepar) (5.29.4)\n",
            "Requirement already satisfied: sentencepiece>=0.1.91 in /usr/local/lib/python3.11/dist-packages (from benepar) (0.2.0)\n",
            "Requirement already satisfied: language-data>=1.2 in /usr/local/lib/python3.11/dist-packages (from langcodes<4.0.0,>=3.2.0->spacy) (1.3.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.11/dist-packages (from nltk>=3.2->benepar) (8.1.8)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.11/dist-packages (from nltk>=3.2->benepar) (1.4.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.11/dist-packages (from nltk>=3.2->benepar) (2024.11.6)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.1 in /usr/local/lib/python3.11/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (2.33.1)\n",
            "Requirement already satisfied: typing-extensions>=4.12.2 in /usr/local/lib/python3.11/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (4.13.1)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (0.4.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2025.1.31)\n",
            "Requirement already satisfied: blis<1.4.0,>=1.3.0 in /usr/local/lib/python3.11/dist-packages (from thinc<8.4.0,>=8.3.4->spacy) (1.3.0)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.11/dist-packages (from thinc<8.4.0,>=8.3.4->spacy) (0.1.5)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.16.4 in /usr/local/lib/python3.11/dist-packages (from tokenizers>=0.9.4->benepar) (0.30.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch>=1.6.0->benepar) (3.18.0)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=1.6.0->benepar) (3.4.2)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch>=1.6.0->benepar) (2025.3.2)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.6.0->benepar) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.6.0->benepar) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.6.0->benepar) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch>=1.6.0->benepar) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch>=1.6.0->benepar) (12.4.5.8)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch>=1.6.0->benepar) (11.2.1.3)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch>=1.6.0->benepar) (10.3.5.147)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch>=1.6.0->benepar) (11.6.1.9)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch>=1.6.0->benepar) (12.3.1.170)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.6.0->benepar) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=1.6.0->benepar) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.6.0->benepar) (12.4.127)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.6.0->benepar) (12.4.127)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.6.0->benepar) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=1.6.0->benepar) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=1.6.0->benepar) (1.3.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers>=4.2.2->transformers[tokenizers,torch]>=4.2.2->benepar) (6.0.2)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers>=4.2.2->transformers[tokenizers,torch]>=4.2.2->benepar) (0.5.3)\n",
            "Requirement already satisfied: accelerate>=0.26.0 in /usr/local/lib/python3.11/dist-packages (from transformers[tokenizers,torch]>=4.2.2->benepar) (1.5.2)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0.0,>=0.3.0->spacy) (1.5.4)\n",
            "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0.0,>=0.3.0->spacy) (13.9.4)\n",
            "Requirement already satisfied: cloudpathlib<1.0.0,>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from weasel<0.5.0,>=0.1.0->spacy) (0.21.0)\n",
            "Requirement already satisfied: smart-open<8.0.0,>=5.2.1 in /usr/local/lib/python3.11/dist-packages (from weasel<0.5.0,>=0.1.0->spacy) (7.1.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->spacy) (3.0.2)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.11/dist-packages (from accelerate>=0.26.0->transformers[tokenizers,torch]>=4.2.2->benepar) (5.9.5)\n",
            "Requirement already satisfied: marisa-trie>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy) (1.2.1)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (2.18.0)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.11/dist-packages (from smart-open<8.0.0,>=5.2.1->weasel<0.5.0,>=0.1.0->spacy) (1.17.2)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (0.1.2)\n",
            "Collecting en-core-web-sm==3.8.0\n",
            "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.8.0/en_core_web_sm-3.8.0-py3-none-any.whl (12.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.8/12.8 MB\u001b[0m \u001b[31m99.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('en_core_web_sm')\n",
            "\u001b[38;5;3m⚠ Restart to reload dependencies\u001b[0m\n",
            "If you are in a Jupyter or Colab notebook, you may need to restart Python in\n",
            "order to load all the package's dependencies. You can do this by selecting the\n",
            "'Restart kernel' or 'Restart runtime' option.\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import pandas as pd\n",
        "from transformers import RobertaTokenizer, RobertaModel\n",
        "from torch.optim import AdamW\n",
        "from sklearn.metrics import f1_score\n",
        "import numpy as np\n",
        "import os\n",
        "!pip install spacy benepar\n",
        "!python -m spacy download en_core_web_sm"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 1. Data preprocessing\n",
        "加载 VUA 数据集，并转换为适用于 RoBERTa 的格式（Tokenization）。\n",
        "\n",
        "处理标签（隐喻/非隐喻分类任务）。\n",
        "\n"
      ],
      "metadata": {
        "id": "6QDenszfkjuO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset Class\n",
        "class MetaphorDataset(Dataset):\n",
        "    def __init__(self, file_path, tokenizer_name=\"roberta-base\", max_len=128):\n",
        "        self.data = pd.read_csv(file_path, sep='\\t', header=None)\n",
        "        self.tokenizer = RobertaTokenizer.from_pretrained(tokenizer_name)\n",
        "        self.max_len = max_len\n",
        "\n",
        "        self.sentences = self.data[2].tolist()\n",
        "        self.fgpos = self.data[4].tolist()\n",
        "        self.target_indices = self.data[5].tolist()\n",
        "        self.labels = self.data[6].tolist()\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.sentences)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        sentence = str(self.sentences[idx])\n",
        "        fgpos_tag = str(self.fgpos[idx])\n",
        "        target_index = int(self.target_indices[idx])\n",
        "        label = int(self.labels[idx])\n",
        "\n",
        "        words = sentence.split()\n",
        "        target_word = words[target_index]\n",
        "\n",
        "        marked_sentence = f\"<s> {' '.join(words)} </s> {fgpos_tag} </s>\"\n",
        "        sent_encoding = self.tokenizer(marked_sentence, padding='max_length', truncation=True, max_length=self.max_len, return_tensors='pt')\n",
        "\n",
        "        isolated_word = f\"<s> {target_word} </s>\"\n",
        "        word_encoding = self.tokenizer(isolated_word, padding='max_length', truncation=True, max_length=self.max_len, return_tensors='pt')\n",
        "\n",
        "        tokenized_words = self.tokenizer.tokenize(\" \".join(words))\n",
        "        try:\n",
        "            token_idx = tokenized_words.index(self.tokenizer.tokenize(target_word)[0]) + 1\n",
        "        except ValueError:\n",
        "            token_idx = 1\n",
        "\n",
        "        return {\n",
        "            \"sentence_input_ids\": sent_encoding[\"input_ids\"].squeeze(0),\n",
        "            \"sentence_attention_mask\": sent_encoding[\"attention_mask\"].squeeze(0),\n",
        "            \"word_input_ids\": word_encoding[\"input_ids\"].squeeze(0),\n",
        "            \"word_attention_mask\": word_encoding[\"attention_mask\"].squeeze(0),\n",
        "            \"target_token_index\": torch.tensor(token_idx),\n",
        "            \"label\": torch.tensor(label, dtype=torch.float)\n",
        "        }"
      ],
      "metadata": {
        "id": "q7oZus_9koIg"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2. Model building\n",
        "加载 RoBERTa 预训练模型。\n",
        "\n",
        "修改最后一层以适应二分类任务（隐喻检测）。"
      ],
      "metadata": {
        "id": "-hOj7S_LkogP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Model using MIP + SPV as in MelBERT\n",
        "class MelBERT(nn.Module):\n",
        "    def __init__(self, hidden_size=768, classifier_hidden=256):\n",
        "        super(MelBERT, self).__init__()\n",
        "        self.encoder = RobertaModel.from_pretrained(\"roberta-base\", add_pooling_layer=False)\n",
        "\n",
        "        self.mip_layer = nn.Linear(2 * hidden_size, classifier_hidden)\n",
        "        self.spv_layer = nn.Linear(2 * hidden_size, classifier_hidden)\n",
        "        self.classifier = nn.Linear(2 * classifier_hidden, 1)\n",
        "        self.dropout = nn.Dropout(0.1)\n",
        "        self.sigmoid = nn.Sigmoid()\n",
        "\n",
        "        self._init_weights(self.mip_layer)\n",
        "        self._init_weights(self.spv_layer)\n",
        "        self._init_weights(self.classifier)\n",
        "\n",
        "    def _init_weights(self, module):\n",
        "        if isinstance(module, nn.Linear):\n",
        "            nn.init.xavier_uniform_(module.weight)\n",
        "            if module.bias is not None:\n",
        "                module.bias.data.zero_()\n",
        "\n",
        "    def forward(self, sentence_input_ids, sentence_attention_mask, word_input_ids, word_attention_mask, target_token_index):\n",
        "        sent_output = self.encoder(input_ids=sentence_input_ids, attention_mask=sentence_attention_mask)\n",
        "        word_output = self.encoder(input_ids=word_input_ids, attention_mask=word_attention_mask)\n",
        "\n",
        "        vS = sent_output.last_hidden_state[:, 0, :]  # sentence CLS\n",
        "        vS_t = torch.stack([sent_output.last_hidden_state[i, idx, :] for i, idx in enumerate(target_token_index)])\n",
        "        vT = word_output.last_hidden_state[:, 0, :]  # isolated CLS\n",
        "\n",
        "        h_mip = self.dropout(torch.relu(self.mip_layer(torch.cat([vT, vS_t], dim=1))))\n",
        "        h_spv = self.dropout(torch.relu(self.spv_layer(torch.cat([vS, vS_t], dim=1))))\n",
        "\n",
        "        logits = self.classifier(torch.cat([h_mip, h_spv], dim=1)).squeeze(1)\n",
        "        return self.sigmoid(logits)\n"
      ],
      "metadata": {
        "id": "OUSDM1g3ks0S"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 3. Model training\n",
        "设定优化器、损失函数（如交叉熵损失）。\n",
        "\n",
        "使用 GPU 训练，并调整超参数（学习率、批量大小等）。"
      ],
      "metadata": {
        "id": "At7QvEeektNf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Training Loop\n",
        "def train(model, dataloader, optimizer, criterion, device):\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "    for batch in dataloader:\n",
        "        for k in batch:\n",
        "            batch[k] = batch[k].to(device)\n",
        "\n",
        "        outputs = model(batch['sentence_input_ids'], batch['sentence_attention_mask'],\n",
        "                        batch['word_input_ids'], batch['word_attention_mask'], batch['target_token_index'])\n",
        "        loss = criterion(outputs, batch['label'])\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        optimizer.zero_grad()\n",
        "        total_loss += loss.item()\n",
        "    return total_loss / len(dataloader)"
      ],
      "metadata": {
        "id": "JPiX-qfpkvZY"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 4. Model evaluation\n",
        "在测试集上进行评估，计算 F1-score, accuracy 等指标。"
      ],
      "metadata": {
        "id": "Z-y__Nhtkvy0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluation Loop\n",
        "def evaluate(model, dataloader, device):\n",
        "    model.eval()\n",
        "    preds, labels = [], []\n",
        "    with torch.no_grad():\n",
        "        for batch in dataloader:\n",
        "            for k in batch:\n",
        "                batch[k] = batch[k].to(device)\n",
        "            outputs = model(batch['sentence_input_ids'], batch['sentence_attention_mask'],\n",
        "                            batch['word_input_ids'], batch['word_attention_mask'], batch['target_token_index'])\n",
        "            preds.extend(outputs.cpu().numpy())\n",
        "            labels.extend(batch['label'].cpu().numpy())\n",
        "    preds = np.array(preds) > 0.5\n",
        "    return f1_score(labels, preds), preds"
      ],
      "metadata": {
        "id": "6fXr3IoYkz6e"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 5. Main"
      ],
      "metadata": {
        "id": "ioLhzFrTk0OU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rX-dwv-JFshK",
        "outputId": "68b113f2-8214-4546-af41-3f077333d7bc"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Main Entry\n",
        "train_file = \"/content/drive/MyDrive/Colab Notebooks/cl dataset/train_novelty_labels.tsv\"\n",
        "test_file = \"/content/drive/MyDrive/Colab Notebooks/cl dataset/test_novelty_labels.tsv\"\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "train_dataset = MetaphorDataset(train_file)\n",
        "test_dataset = MetaphorDataset(test_file)\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
        "test_loader = DataLoader(test_dataset, batch_size=32)\n",
        "\n",
        "model = MelBERT().to(device)\n",
        "optimizer = AdamW(model.parameters(), lr=2e-5)\n",
        "criterion = nn.BCELoss()\n",
        "\n",
        "for epoch in range(5):\n",
        "    train_loss = train(model, train_loader, optimizer, criterion, device)\n",
        "    f1, _ = evaluate(model, test_loader, device)\n",
        "    print(f\"Epoch {epoch+1} - Train Loss: {train_loss:.4f}, Test F1: {f1:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 332
        },
        "id": "qVbYM4o_lBbs",
        "outputId": "cd05b399-010b-4d11-b41b-ccccd80c4dff"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: 'config.json'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-8-64fbada5199f>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"__main__\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m     \u001b[0mconfig\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_config\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"config.json\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0mtrain_file\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"train_file\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-8-64fbada5199f>\u001b[0m in \u001b[0;36mload_config\u001b[0;34m(config_path)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Main Entry\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mload_config\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig_path\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"/content/drive/MyDrive/Colab Notebooks/cl dataset/config.json\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'r'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mjson\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'config.json'"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# extraction"
      ],
      "metadata": {
        "id": "yHvOQhEGDfre"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "import benepar\n",
        "\n",
        "# 加载 parser\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "if not benepar.is_loaded(\"benepar_en3\"):\n",
        "    benepar.download(\"benepar_en3\")\n",
        "nlp.add_pipe(\"benepar\", config={\"model\": \"benepar_en3\"})\n",
        "\n",
        "def extract_constituent_phrase(sentence, target_index, phrase_type=\"NP_or_VP\"):\n",
        "    \"\"\"\n",
        "    phrase_type = \"NP_or_VP\": 自动判断\n",
        "                 \"NP\": 强制只抽NP\n",
        "                 \"VP\": 强制只抽VP\n",
        "    \"\"\"\n",
        "    doc = nlp(sentence)\n",
        "    words = sentence.split()\n",
        "    target_word = words[target_index]\n",
        "\n",
        "    for sent in doc.sents:\n",
        "        tree = sent._.parse_string\n",
        "        parsed = sent._.constituent\n",
        "\n",
        "        # 遍历 constituent 树查找包含 target 的最小 NP/VP\n",
        "        for constituent in parsed.subtrees:\n",
        "            if constituent.label_ in {\"NP\", \"VP\"}:\n",
        "                leaves = list(constituent.leaves())\n",
        "                leaf_text = [t.text for t in leaves]\n",
        "                if target_word in leaf_text:\n",
        "                    if phrase_type == \"NP_or_VP\":\n",
        "                        return \" \".join(leaf_text)\n",
        "                    elif phrase_type == \"NP\" and constituent.label_ == \"NP\":\n",
        "                        return \" \".join(leaf_text)\n",
        "                    elif phrase_type == \"VP\" and constituent.label_ == \"VP\":\n",
        "                        return \" \".join(leaf_text)\n",
        "    return target_word  # fallback\n"
      ],
      "metadata": {
        "id": "NTKE66QMDckE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def guess_phrase_type(pos_tag):\n",
        "    if pos_tag.startswith(\"V\"):\n",
        "        return \"VP\"\n",
        "    elif pos_tag.startswith(\"N\"):\n",
        "        return \"NP\"\n",
        "    else:\n",
        "        return \"NP_or_VP\"\n",
        "\n",
        "\n",
        "# 输出模型预测为 metaphor 的词及其所在短语\n",
        "for i in range(len(test_dataset)):\n",
        "    item = test_dataset[i]\n",
        "    sentence = test_dataset.sentences[i]\n",
        "    idx = int(test_dataset.target_indices[i])\n",
        "    pos = test_dataset.pos[i]\n",
        "    if preds[i]:  # 被预测为 metaphor\n",
        "        phrase = extract_constituent_phrase(sentence, idx, guess_phrase_type(pos))\n",
        "        print(f\"📍 [{phrase}]  ← from sentence: {sentence}\")\n"
      ],
      "metadata": {
        "id": "O_2BQjFJDrxe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# save"
      ],
      "metadata": {
        "id": "-4ZYHFcgD0cU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import csv\n",
        "\n",
        "output_path = \"melbert_metaphor_predictions.tsv\"\n",
        "rows = []\n",
        "\n",
        "for i in range(len(test_dataset)):\n",
        "    sentence = test_dataset.sentences[i]\n",
        "    idx = int(test_dataset.target_indices[i])\n",
        "    pos = test_dataset.pos[i]\n",
        "    label = int(test_dataset.labels[i])\n",
        "    pred = int(preds[i])\n",
        "\n",
        "    target_word = sentence.split()[idx]\n",
        "    phrase = extract_constituent_phrase(sentence, idx, guess_phrase_type(pos))\n",
        "\n",
        "    rows.append({\n",
        "        \"sentence\": sentence,\n",
        "        \"target_word\": target_word,\n",
        "        \"target_index\": idx,\n",
        "        \"POS\": pos,\n",
        "        \"true_label\": label,\n",
        "        \"predicted_label\": pred,\n",
        "        \"extracted_phrase\": phrase\n",
        "    })\n",
        "\n",
        "# 保存为 TSV\n",
        "with open(output_path, \"w\", newline=\"\", encoding=\"utf-8\") as f:\n",
        "    writer = csv.DictWriter(f, fieldnames=rows[0].keys(), delimiter=\"\\t\")\n",
        "    writer.writeheader()\n",
        "    writer.writerows(rows)\n",
        "\n",
        "print(f\"✅ 保存成功！预测结果已写入：{output_path}\")"
      ],
      "metadata": {
        "id": "DqNWT80ED1ye"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}