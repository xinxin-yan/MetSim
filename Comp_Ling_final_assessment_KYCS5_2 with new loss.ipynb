{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/xinxin-yan/MetSim/blob/main/Comp_Ling_final_assessment_KYCS5_2%20with%20new%20loss.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IiYBxe0dGsgy"
      },
      "source": [
        "# 0. Encironment preparation\n",
        "ÂÆâË£Ö transformers, datasets, torch Á≠â‰æùËµñÂ∫ì„ÄÇ"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "RWHyzbdWa3Du",
        "outputId": "48d71cae-5b30-4aaf-f878-f8659e071c86"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: spacy in /usr/local/lib/python3.11/dist-packages (3.7.5)\n",
            "Requirement already satisfied: benepar in /usr/local/lib/python3.11/dist-packages (0.2.0)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.11/dist-packages (from spacy) (3.0.12)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (1.0.5)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (1.0.12)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.11/dist-packages (from spacy) (2.0.11)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.11/dist-packages (from spacy) (3.0.9)\n",
            "Requirement already satisfied: thinc<8.3.0,>=8.2.2 in /usr/local/lib/python3.11/dist-packages (from spacy) (8.2.5)\n",
            "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.11/dist-packages (from spacy) (1.1.3)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.11/dist-packages (from spacy) (2.5.1)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.11/dist-packages (from spacy) (2.0.10)\n",
            "Requirement already satisfied: weasel<0.5.0,>=0.1.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (0.4.1)\n",
            "Requirement already satisfied: typer<1.0.0,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (0.15.2)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (4.67.1)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (2.32.3)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /usr/local/lib/python3.11/dist-packages (from spacy) (2.10.6)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from spacy) (3.1.5)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from spacy) (75.1.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (24.2)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (3.5.0)\n",
            "Requirement already satisfied: numpy>=1.19.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (1.26.4)\n",
            "Requirement already satisfied: nltk>=3.2 in /usr/local/lib/python3.11/dist-packages (from benepar) (3.9.1)\n",
            "Requirement already satisfied: torch>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from benepar) (2.5.1+cu124)\n",
            "Requirement already satisfied: torch-struct>=0.5 in /usr/local/lib/python3.11/dist-packages (from benepar) (0.5)\n",
            "Requirement already satisfied: tokenizers>=0.9.4 in /usr/local/lib/python3.11/dist-packages (from benepar) (0.21.0)\n",
            "Requirement already satisfied: transformers>=4.2.2 in /usr/local/lib/python3.11/dist-packages (from transformers[tokenizers,torch]>=4.2.2->benepar) (4.48.3)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.11/dist-packages (from benepar) (4.25.6)\n",
            "Requirement already satisfied: sentencepiece>=0.1.91 in /usr/local/lib/python3.11/dist-packages (from benepar) (0.2.0)\n",
            "Requirement already satisfied: language-data>=1.2 in /usr/local/lib/python3.11/dist-packages (from langcodes<4.0.0,>=3.2.0->spacy) (1.3.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.11/dist-packages (from nltk>=3.2->benepar) (8.1.8)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.11/dist-packages (from nltk>=3.2->benepar) (1.4.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.11/dist-packages (from nltk>=3.2->benepar) (2024.11.6)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.27.2 in /usr/local/lib/python3.11/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (2.27.2)\n",
            "Requirement already satisfied: typing-extensions>=4.12.2 in /usr/local/lib/python3.11/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2025.1.31)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.11/dist-packages (from thinc<8.3.0,>=8.2.2->spacy) (0.7.11)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.11/dist-packages (from thinc<8.3.0,>=8.2.2->spacy) (0.1.5)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.16.4 in /usr/local/lib/python3.11/dist-packages (from tokenizers>=0.9.4->benepar) (0.28.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch>=1.6.0->benepar) (3.17.0)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=1.6.0->benepar) (3.4.2)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch>=1.6.0->benepar) (2024.10.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.6.0->benepar) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.6.0->benepar) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.6.0->benepar) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch>=1.6.0->benepar) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch>=1.6.0->benepar) (12.4.5.8)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch>=1.6.0->benepar) (11.2.1.3)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch>=1.6.0->benepar) (10.3.5.147)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch>=1.6.0->benepar) (11.6.1.9)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch>=1.6.0->benepar) (12.3.1.170)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=1.6.0->benepar) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.6.0->benepar) (12.4.127)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.6.0->benepar) (12.4.127)\n",
            "Requirement already satisfied: triton==3.1.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.6.0->benepar) (3.1.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=1.6.0->benepar) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=1.6.0->benepar) (1.3.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers>=4.2.2->transformers[tokenizers,torch]>=4.2.2->benepar) (6.0.2)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.11/dist-packages (from transformers>=4.2.2->transformers[tokenizers,torch]>=4.2.2->benepar) (0.5.3)\n",
            "Requirement already satisfied: accelerate>=0.26.0 in /usr/local/lib/python3.11/dist-packages (from transformers[tokenizers,torch]>=4.2.2->benepar) (1.3.0)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0.0,>=0.3.0->spacy) (1.5.4)\n",
            "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0.0,>=0.3.0->spacy) (13.9.4)\n",
            "Requirement already satisfied: cloudpathlib<1.0.0,>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from weasel<0.5.0,>=0.1.0->spacy) (0.20.0)\n",
            "Requirement already satisfied: smart-open<8.0.0,>=5.2.1 in /usr/local/lib/python3.11/dist-packages (from weasel<0.5.0,>=0.1.0->spacy) (7.1.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->spacy) (3.0.2)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.11/dist-packages (from accelerate>=0.26.0->transformers[tokenizers,torch]>=4.2.2->benepar) (5.9.5)\n",
            "Requirement already satisfied: marisa-trie>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy) (1.2.1)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (2.18.0)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.11/dist-packages (from smart-open<8.0.0,>=5.2.1->weasel<0.5.0,>=0.1.0->spacy) (1.17.2)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (0.1.2)\n",
            "Collecting en-core-web-sm==3.7.1\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import pandas as pd\n",
        "from transformers import RobertaTokenizer, RobertaModel\n",
        "from torch.optim import AdamW\n",
        "from sklearn.metrics import f1_score\n",
        "import numpy as np\n",
        "import os\n",
        "!pip install spacy benepar\n",
        "!python -m spacy download en_core_web_sm"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6QDenszfkjuO"
      },
      "source": [
        "# 1. Data preprocessing\n",
        "Âä†ËΩΩ VUA Êï∞ÊçÆÈõÜÔºåÂπ∂ËΩ¨Êç¢‰∏∫ÈÄÇÁî®‰∫é RoBERTa ÁöÑÊ†ºÂºèÔºàTokenizationÔºâ„ÄÇ\n",
        "\n",
        "Â§ÑÁêÜÊ†áÁ≠æÔºàÈöêÂñª/ÈùûÈöêÂñªÂàÜÁ±ª‰ªªÂä°Ôºâ„ÄÇ\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "q7oZus_9koIg"
      },
      "outputs": [],
      "source": [
        "# Dataset Class\n",
        "class MetaphorDataset(Dataset):\n",
        "    def __init__(self, file_path, tokenizer_name=\"roberta-base\", max_len=128):\n",
        "        self.data = pd.read_csv(file_path, sep='\\t', header=None)\n",
        "        self.tokenizer = RobertaTokenizer.from_pretrained(tokenizer_name)\n",
        "        self.max_len = max_len\n",
        "\n",
        "        self.sentences = self.data[2].tolist()\n",
        "        self.fgpos = self.data[4].tolist()\n",
        "        self.target_indices = self.data[5].tolist()\n",
        "        self.labels = self.data[6].tolist()\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.sentences)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        sentence = str(self.sentences[idx])\n",
        "        fgpos_tag = str(self.fgpos[idx])\n",
        "        target_index = int(self.target_indices[idx])\n",
        "        label = int(self.labels[idx])\n",
        "\n",
        "        words = sentence.split()\n",
        "        target_word = words[target_index]\n",
        "\n",
        "        marked_sentence = f\"<s> {' '.join(words)} </s> {fgpos_tag} </s>\"\n",
        "        sent_encoding = self.tokenizer(marked_sentence, padding='max_length', truncation=True, max_length=self.max_len, return_tensors='pt')\n",
        "\n",
        "        isolated_word = f\"<s> {target_word} </s>\"\n",
        "        word_encoding = self.tokenizer(isolated_word, padding='max_length', truncation=True, max_length=self.max_len, return_tensors='pt')\n",
        "\n",
        "        tokenized_words = self.tokenizer.tokenize(\" \".join(words))\n",
        "        try:\n",
        "            token_idx = tokenized_words.index(self.tokenizer.tokenize(target_word)[0]) + 1\n",
        "        except ValueError:\n",
        "            token_idx = 1\n",
        "\n",
        "        return {\n",
        "            \"sentence_input_ids\": sent_encoding[\"input_ids\"].squeeze(0),\n",
        "            \"sentence_attention_mask\": sent_encoding[\"attention_mask\"].squeeze(0),\n",
        "            \"word_input_ids\": word_encoding[\"input_ids\"].squeeze(0),\n",
        "            \"word_attention_mask\": word_encoding[\"attention_mask\"].squeeze(0),\n",
        "            \"target_token_index\": torch.tensor(token_idx),\n",
        "            \"label\": torch.tensor(label, dtype=torch.float)\n",
        "        }"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-hOj7S_LkogP"
      },
      "source": [
        "# 2. Model building\n",
        "Âä†ËΩΩ RoBERTa È¢ÑËÆ≠ÁªÉÊ®°Âûã„ÄÇ\n",
        "\n",
        "‰øÆÊîπÊúÄÂêé‰∏ÄÂ±Ç‰ª•ÈÄÇÂ∫î‰∫åÂàÜÁ±ª‰ªªÂä°ÔºàÈöêÂñªÊ£ÄÊµãÔºâ„ÄÇ"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OUSDM1g3ks0S"
      },
      "outputs": [],
      "source": [
        "# Model using MIP + SPV as in MelBERT\n",
        "class MelBERT(nn.Module):\n",
        "    def __init__(self, hidden_size=768, classifier_hidden=256):\n",
        "        super(MelBERT, self).__init__()\n",
        "        self.encoder = RobertaModel.from_pretrained(\"roberta-base\", add_pooling_layer=False)\n",
        "\n",
        "        self.mip_layer = nn.Linear(2 * hidden_size, classifier_hidden)\n",
        "        self.spv_layer = nn.Linear(2 * hidden_size, classifier_hidden)\n",
        "        self.classifier = nn.Linear(2 * classifier_hidden, 1)\n",
        "        self.dropout = nn.Dropout(0.1)\n",
        "        self.sigmoid = nn.Sigmoid()\n",
        "\n",
        "        self._init_weights(self.mip_layer)\n",
        "        self._init_weights(self.spv_layer)\n",
        "        self._init_weights(self.classifier)\n",
        "\n",
        "    def _init_weights(self, module):\n",
        "        if isinstance(module, nn.Linear):\n",
        "            nn.init.xavier_uniform_(module.weight)\n",
        "            if module.bias is not None:\n",
        "                module.bias.data.zero_()\n",
        "\n",
        "    def forward(self, sentence_input_ids, sentence_attention_mask, word_input_ids, word_attention_mask, target_token_index):\n",
        "        sent_output = self.encoder(input_ids=sentence_input_ids, attention_mask=sentence_attention_mask)\n",
        "        word_output = self.encoder(input_ids=word_input_ids, attention_mask=word_attention_mask)\n",
        "\n",
        "        vS = sent_output.last_hidden_state[:, 0, :]  # sentence CLS\n",
        "        vS_t = torch.stack([sent_output.last_hidden_state[i, idx, :] for i, idx in enumerate(target_token_index)])\n",
        "        vT = word_output.last_hidden_state[:, 0, :]  # isolated CLS\n",
        "\n",
        "        h_mip = self.dropout(torch.relu(self.mip_layer(torch.cat([vT, vS_t], dim=1))))\n",
        "        h_spv = self.dropout(torch.relu(self.spv_layer(torch.cat([vS, vS_t], dim=1))))\n",
        "\n",
        "        logits = self.classifier(torch.cat([h_mip, h_spv], dim=1)).squeeze(1)\n",
        "        return self.sigmoid(logits)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "At7QvEeektNf"
      },
      "source": [
        "# 3. Model training\n",
        "ËÆæÂÆö‰ºòÂåñÂô®„ÄÅÊçüÂ§±ÂáΩÊï∞ÔºàÂ¶Ç‰∫§ÂèâÁÜµÊçüÂ§±Ôºâ„ÄÇ\n",
        "\n",
        "‰ΩøÁî® GPU ËÆ≠ÁªÉÔºåÂπ∂Ë∞ÉÊï¥Ë∂ÖÂèÇÊï∞ÔºàÂ≠¶‰π†Áéá„ÄÅÊâπÈáèÂ§ßÂ∞èÁ≠âÔºâ„ÄÇ"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JPiX-qfpkvZY"
      },
      "outputs": [],
      "source": [
        "# Training Loop\n",
        "def train(model, dataloader, optimizer, device, class_weight=1.0):\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "\n",
        "    # Weighted NLLLoss\n",
        "    loss_fct = nn.BCEWithLogitsLoss(pos_weight=torch.tensor([class_weight]).to(device))\n",
        "\n",
        "    for batch in dataloader:\n",
        "        for k in batch:\n",
        "            batch[k] = batch[k].to(device)\n",
        "\n",
        "        outputs = model(batch['sentence_input_ids'],\n",
        "                        batch['sentence_attention_mask'],\n",
        "                        batch['word_input_ids'],\n",
        "                        batch['word_attention_mask'],\n",
        "                        batch['target_token_index'])\n",
        "\n",
        "        # If your model ends with sigmoid(), use BCELoss instead\n",
        "        logits = torch.stack([1 - outputs, outputs], dim=1)  # [batch_size, 2] for NLLLoss\n",
        "        labels = batch['label'].float()  # float for BCE loss\n",
        "\n",
        "        loss = loss_fct(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        optimizer.zero_grad()\n",
        "        total_loss += loss.item()\n",
        "\n",
        "    return total_loss / len(dataloader)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z-y__Nhtkvy0"
      },
      "source": [
        "# 4. Model evaluation\n",
        "Âú®ÊµãËØïÈõÜ‰∏äËøõË°åËØÑ‰º∞ÔºåËÆ°ÁÆó F1-score, accuracy Á≠âÊåáÊ†á„ÄÇ"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6fXr3IoYkz6e"
      },
      "outputs": [],
      "source": [
        "# Evaluation Loop\n",
        "def evaluate(model, dataloader, device):\n",
        "    model.eval()\n",
        "    preds, labels = [], []\n",
        "    with torch.no_grad():\n",
        "        for batch in dataloader:\n",
        "            for k in batch:\n",
        "                batch[k] = batch[k].to(device)\n",
        "            outputs = model(batch['sentence_input_ids'], batch['sentence_attention_mask'],\n",
        "                            batch['word_input_ids'], batch['word_attention_mask'], batch['target_token_index'])\n",
        "            preds.extend(outputs.cpu().numpy())\n",
        "            labels.extend(batch['label'].cpu().numpy())\n",
        "    preds = np.array(preds) > 0.5\n",
        "    return f1_score(labels, preds), preds"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ioLhzFrTk0OU"
      },
      "source": [
        "# 5. Main"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rX-dwv-JFshK",
        "outputId": "56300f44-679d-4e72-eb2a-64358984eeeb"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "qVbYM4o_lBbs",
        "outputId": "69fc7653-c72e-45db-fd2d-627e28b3b0d5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1 - Train Loss: 0.7064, Test F1: 0.0000\n",
            "Epoch 2 - Train Loss: 0.7060, Test F1: 0.0000\n",
            "Epoch 3 - Train Loss: 0.7060, Test F1: 0.0000\n",
            "Epoch 4 - Train Loss: 0.7060, Test F1: 0.0000\n",
            "Epoch 5 - Train Loss: 0.7060, Test F1: 0.0000\n"
          ]
        }
      ],
      "source": [
        "# Main Entry\n",
        "train_file = \"/content/drive/MyDrive/Colab Notebooks/cl dataset/train_novelty_labels.tsv\"\n",
        "test_file = \"/content/drive/MyDrive/Colab Notebooks/cl dataset/test_novelty_labels.tsv\"\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "train_dataset = MetaphorDataset(train_file)\n",
        "test_dataset = MetaphorDataset(test_file)\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
        "test_loader = DataLoader(test_dataset, batch_size=32)\n",
        "\n",
        "model = MelBERT().to(device)\n",
        "optimizer = AdamW(model.parameters(), lr=2e-5)\n",
        "criterion = nn.BCELoss()\n",
        "\n",
        "for epoch in range(5):\n",
        "    train_loss = train(model, train_loader, optimizer, device, class_weight=5.0)\n",
        "    f1, _ = evaluate(model, test_loader, device)\n",
        "    print(f\"Epoch {epoch+1} - Train Loss: {train_loss:.4f}, Test F1: {f1:.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yHvOQhEGDfre"
      },
      "source": [
        "# extraction"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NTKE66QMDckE"
      },
      "outputs": [],
      "source": [
        "import spacy\n",
        "import benepar\n",
        "\n",
        "# Âä†ËΩΩ parser\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "if not benepar.is_loaded(\"benepar_en3\"):\n",
        "    benepar.download(\"benepar_en3\")\n",
        "nlp.add_pipe(\"benepar\", config={\"model\": \"benepar_en3\"})\n",
        "\n",
        "def extract_constituent_phrase(sentence, target_index, phrase_type=\"NP_or_VP\"):\n",
        "    \"\"\"\n",
        "    phrase_type = \"NP_or_VP\": Ëá™Âä®Âà§Êñ≠\n",
        "                 \"NP\": Âº∫Âà∂Âè™ÊäΩNP\n",
        "                 \"VP\": Âº∫Âà∂Âè™ÊäΩVP\n",
        "    \"\"\"\n",
        "    doc = nlp(sentence)\n",
        "    words = sentence.split()\n",
        "    target_word = words[target_index]\n",
        "\n",
        "    for sent in doc.sents:\n",
        "        tree = sent._.parse_string\n",
        "        parsed = sent._.constituent\n",
        "\n",
        "        # ÈÅçÂéÜ constituent Ê†ëÊü•ÊâæÂåÖÂê´ target ÁöÑÊúÄÂ∞è NP/VP\n",
        "        for constituent in parsed.subtrees:\n",
        "            if constituent.label_ in {\"NP\", \"VP\"}:\n",
        "                leaves = list(constituent.leaves())\n",
        "                leaf_text = [t.text for t in leaves]\n",
        "                if target_word in leaf_text:\n",
        "                    if phrase_type == \"NP_or_VP\":\n",
        "                        return \" \".join(leaf_text)\n",
        "                    elif phrase_type == \"NP\" and constituent.label_ == \"NP\":\n",
        "                        return \" \".join(leaf_text)\n",
        "                    elif phrase_type == \"VP\" and constituent.label_ == \"VP\":\n",
        "                        return \" \".join(leaf_text)\n",
        "    return target_word  # fallback\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "O_2BQjFJDrxe"
      },
      "outputs": [],
      "source": [
        "def guess_phrase_type(pos_tag):\n",
        "    if pos_tag.startswith(\"V\"):\n",
        "        return \"VP\"\n",
        "    elif pos_tag.startswith(\"N\"):\n",
        "        return \"NP\"\n",
        "    else:\n",
        "        return \"NP_or_VP\"\n",
        "\n",
        "\n",
        "# ËæìÂá∫Ê®°ÂûãÈ¢ÑÊµã‰∏∫ metaphor ÁöÑËØçÂèäÂÖ∂ÊâÄÂú®Áü≠ËØ≠\n",
        "for i in range(len(test_dataset)):\n",
        "    item = test_dataset[i]\n",
        "    sentence = test_dataset.sentences[i]\n",
        "    idx = int(test_dataset.target_indices[i])\n",
        "    pos = test_dataset.pos[i]\n",
        "    if preds[i]:  # Ë¢´È¢ÑÊµã‰∏∫ metaphor\n",
        "        phrase = extract_constituent_phrase(sentence, idx, guess_phrase_type(pos))\n",
        "        print(f\"üìç [{phrase}]  ‚Üê from sentence: {sentence}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-4ZYHFcgD0cU"
      },
      "source": [
        "# save"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DqNWT80ED1ye"
      },
      "outputs": [],
      "source": [
        "import csv\n",
        "\n",
        "output_path = \"melbert_metaphor_predictions.tsv\"\n",
        "rows = []\n",
        "\n",
        "for i in range(len(test_dataset)):\n",
        "    sentence = test_dataset.sentences[i]\n",
        "    idx = int(test_dataset.target_indices[i])\n",
        "    pos = test_dataset.pos[i]\n",
        "    label = int(test_dataset.labels[i])\n",
        "    pred = int(preds[i])\n",
        "\n",
        "    target_word = sentence.split()[idx]\n",
        "    phrase = extract_constituent_phrase(sentence, idx, guess_phrase_type(pos))\n",
        "\n",
        "    rows.append({\n",
        "        \"sentence\": sentence,\n",
        "        \"target_word\": target_word,\n",
        "        \"target_index\": idx,\n",
        "        \"POS\": pos,\n",
        "        \"true_label\": label,\n",
        "        \"predicted_label\": pred,\n",
        "        \"extracted_phrase\": phrase\n",
        "    })\n",
        "\n",
        "# ‰øùÂ≠ò‰∏∫ TSV\n",
        "with open(output_path, \"w\", newline=\"\", encoding=\"utf-8\") as f:\n",
        "    writer = csv.DictWriter(f, fieldnames=rows[0].keys(), delimiter=\"\\t\")\n",
        "    writer.writeheader()\n",
        "    writer.writerows(rows)\n",
        "\n",
        "print(f\"‚úÖ ‰øùÂ≠òÊàêÂäüÔºÅÈ¢ÑÊµãÁªìÊûúÂ∑≤ÂÜôÂÖ•Ôºö{output_path}\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "mount_file_id": "1QQvb0DOHY7wA3LvXJpjCZlx9KQJj4SjQ",
      "authorship_tag": "ABX9TyOCEvvEAPWB5RS6iPS2PvmA",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}